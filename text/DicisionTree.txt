결정트리

과대적합을 방지 하기 위해 최소값을 0에서 3개까지 늘림(데이터의 개수에 따라 불편함)

비율을 정해서 데이터의 일정 비율(불순도)를 조절해서 조절 
혹은
깊이를 설정해서 최대 깊이를 제한함. 
혹은
마지막 노드(리프노드) 개수를 정해줌

• 불순도
개체들의 무질서함을 측정하는 지표 

지니 불순도
• 정답이 아닌 다른 라벨이 뽑힐 확률
• 파란 구슬만 있으면 0, 빨간 구슬만 있어도 0
• 반반 있으면 불순도가 제일 높은 0.5 (제일 균등할 때) 

엔트로피
지니 불순도와 비슷한데 0부터 1 (급격한 데이터 나타날때 사용)


(Gini Impurity = 0.5) 
사과의 비율: 50% (0.5), 바나나의 비율: 50% (0.5) • 지니 불순도 계산: G=1−[(0.5)2+(0.5)2] = 0.5

결정트리 알고리즘 파라미터 설정
• 엔트로피가 최소, 불순도를 과대적합을 피하는 한 0에 가깝게 만드는 방향으로 학습
• Root Node(처음 나누는 것) 에서 데이터를 두 그룹으로 나눌 때 불순도가 0에 가깝게 최대한

-> 구현하기 쉬워서 선호되는 방식이지만 인공지능에선 사람에게 쉬울 필요가 없음. 컴퓨터가 코딩/ 자주 안 씀 


화이트 박스 모델

블랙 박스 모델

시험( 파라미터 별 그래프) 

결정트리의 문제들

회귀할 때 어쩔 수 없이(직선) 같은 구간이 있음

결정 트리는 계단 모양의 결정 경계를 만듦(같은 데이터인데 살짝 축이 바뀌면 구분도 바뀜) 수직, 수평 데이터만 가능
전처리를 통해 값의 방향을 바꿔주게 됨(번거로움)

분산이 상당히 크다는 점
하이퍼파라미터나 데이터를 조금만 변경해도 매우 다른 모델이 생성


가장 성능을 높이는 방법
GridSearchCV를 사용하기.
+ 각 테스트 세트 샘플에 대해 1,000개의 결정 트리예측을 만들고 다수로 나온 예측만 취하면(사이파이의 mode() 함수를 사용), 테스트세트에 대한 다수결 예측(majority-vote prediction)이 만들어짐
데이터를 쪼개고 쪼개서 더하는 기법











