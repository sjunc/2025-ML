K-평균
센트로이드 개수와 위치를 정해줌
밀집도가 아닌 센트로이드 중심 거리 기준이여서 [원형] 특이한 형태에서 한계가 존재

DBSCAN
핵심 샘플 기준으로 거리 인 ε 내에 샘플이 몇 개 놓여 있는지 계산 이웃끼리 합침
ε -이웃 내에 적어도 min_samples개 샘플이 있다면 이를 핵심샘플core instance로 간주
최소 샘플 기준에 없으면 새로운 그룹으로 묶임.
단점
(센트로이트 - 기준점)이 없어서 아예 다른 특성이라도 단순 밀집도 기준이여서 같은 그룹으로 묶일 수 있음

predict() 메서드를 제공하지 않고 fit_predict() 메서드 사용

다양한 비지도 학습이 존재

가우스 혼합, 
이상치와 특이치 탐지, 라벨링을 위한 기법들 

기말 정리

중간 고사 시험 변형 문제 (같은 범위) 20%



결정트리
앙상블학습
차원축소
비지도학습

서술형 큰 틀에서 작성할 수 있게 출제

시험은 시험이기 때문에 개인적 발전을 위해선 공부가 필요하다.

결정트리
하이퍼 파라미터 ( 트리 갯수, 끝낼 데이터 수) 
결정 트리 깊이 등등

지니 불순도 (얼마나 데이터 잘 섞여있나?)
반반일 때 가장 높음. 0.5
0일 때 가장 완벽하게 분리 
=> 지니불순도 : 순수하게 분류되면 0, 완벽하게 섞이면(5:5) 0.5
=> 엔트로피 : 순수하게 분류되면 0, 완벽하게 섞이면(5:5) 1  값의 차이를 더 상세히 알수있음.

결정트리 알고리즘은 사람이 이해하기 쉬우나 성능 자체가 낮음
• 화이트박스(white box) 모델
• 결정 트리와 같이 직관적이고 결정 방식을 이해하기 쉬운 모델

단점
결정트리 회귀 - 선형적이라서 좋지않음
데이터 - 축 방향성만 바뀌여도 계단 모양의 결정 경계를 만듦 

앙상블

투표기반 분류기 
분류기 여러 개를 훈련시켰다고 가정: 로지스틱 회귀 분류기, SVM 분류기, 랜덤 포레스트 분류기, K-최근접 이웃 분류기 등
하드 1/ 0 소프트(간접 투표) 확률 
강한 학습기/ 약한 학습기

용어 시험.
배깅(bagging, bootstrap aggregating의 줄임말)
• 훈련 세트에서 중복을 허용하여 샘플링하는 방식
OOB(out-of-bag) 샘플 - 선택되지 않은 나머지 37%의 샘플 | oob_score=True로 지정하면 훈련이 끝난 후 자동으로 OOB 평가를 수행

페이스팅(pasting)
• 중복을 허용하지 않고 샘플링하는 방식

특성 중요도(-> 차원축소 주성분분석)
• 랜덤 포레스트


부스팅(boosting)
약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법
AdaBoost(adaptive boosting, ‘에이다부스트’)와그레이디언트 부스팅


스태킹
블렌더(blender) 또는 메타 학습기(meta learner)
처음 학습해서 나온 결과를 다시 원본을 마지막에 한 번더 재훈련




차원 축소 
투영
• 모든 훈련 샘플이 고차원 공간 안의 저 차원 부분 공간
데이터셋의 차원을 3D에서 2D로 축소

스위스 롤(Swiss roll) 데이터셋
데이터를 펼쳐서 2D 데이터셋

매니폴드 학습
d차원 매니폴드
• 국부적으로 d차원 초평면(hyperplane)으로 보일 수 있는 n차원 공간의 일부(d < n) • 스위스 롤의 경우에는 d=2이고 n=3
매니폴드 학습
• 매니폴드 가정 또는 매니폴드 가설
• 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 가설

매니폴드 가정에 대비되는 그림


주성분
• PCA는 훈련 세트에서 분산이 최대인 축을 찾아감
• 고차원 데이터셋이라면 PCA는 이전의 두 축에 직교하는 세 번째 축을 찾으며 데이터셋에 있는 차원의 수만큼 네 번째, 다섯 번째, ..., n번째축을찾음
• i번째 축을 이 데이터의 i번째 주성분(principal component, PC)이라고 함

n_components 
숫자가 1이상 차원수, 1이하 오차 허용  0.95 면 5% 오차 허용
훨씬 차원을 많이 줄일 수 있

랜덤 PCA – 확률적 알고리즘

MDS, Isomap, t-SNE
서로 극단 MDS <-> t-SNE

k -평균
최적의 클러스터 엘보구간
실루엣 계수 확인 

DBSCAN 

비지도에선 2가지 k-평균, DBSCAN 










